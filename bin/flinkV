#!/usr/bin/python3
# flinkV: N-gram bag-of-words vector cosine similarity utility
# Copyright (C) 2019 M. Zhou <lumin@debian.org>
# License: MIT/Expat
from typing import *
import argparse, re, os, sys, json, glob, math, random, pprint
from collections import Counter, defaultdict
import numpy as np
from scipy import sparse
import nltk
'''
flinkV implements a (toy) text classifier with the k-NN model (k=1), using
N-gram bag-of-words representation vectors as text representation, and cosine
similarity as the metric.

Keywords: k-NN (machine learning), Bag-of-Words (computational linguistics),
          N-gram (computational linguistics), vector space (linear algebra)

#### Problem Formulation

```latex
Given a sequence of tokens $t = [t_1, t_2, \ldots, t_M]$, we want to classify
the sequence and produce a corresponding output $c$.
```

## Known problems

1. this toy is still not properly assessed. people on IRC suggested me test
it against chromium, fpc, lazarus, boost

2. currently it works not quite well with very short license declarations,
such as the one on the python script itself `:-(`.

3. we need choose a threshold under which the program rejects to classify.

## Manual training and prediction

./licensecheck-ng.py --train datadir/
./licensecheck-ng.py --predict my_file

## Comparison to related works

https://wiki.debian.org/CopyrightReviewTools

* licensecheck
* scan-copyrights
* cme
* licensecheck2dep5
* license-reconcile
* debmake
* decopy
* license
* check-all-the-things
* cargo-lichking
* python-debian
* license finder
* licensed
* ninka
* scancode
* dlt
* deb-pkg-tools
* jninka
* apache-rat
* fossology
* OSLCv3
* https://github.com/nexB/scancode-toolkit
  The core algorithm is similar to what scancode-toolkit called "match set":
  https://github.com/nexB/scancode-toolkit/blob/develop/src/licensedcode/match_set.py
'''

class Model(object):
    '''
    Model that leverages N-gram Bag-of-Words Representation. Vectors are
    compared with k-NN (k=1) with cosine distance as the metric.
    '''
    def __init__(self):
        # Ordered vocab list of every-grams.
        # exampe: [('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
        self.vocab = list()
        # labels, len(labels)==C . Each label corresponds to a row of bow matrix.
        self.labels = list()
        # matrix formed by every-grams bag-of-words vectors. matrix.shape[0]==C
        self.bow = sparse.dok_matrix((1,1), dtype=np.int)

    def save(self, name: str):
        '''
        save the model to <name>.json and <name>.npz
        '''
        with open(name + '.json', 'wt') as f:
            json.dump({'vocab': self.vocab, 'labels': self.labels}, f, indent=2)
        np.savez(name + '.npz', bow=self.bow.todense())

    def load(self, path: str):
        '''
        load the model from <path>.json and <path>.npz
        '''
        with open(path + '.json', 'rt') as f:
            learned = json.load(f)
        self.vocab = list(tuple(x) for x in learned['vocab'])
        self.labels = learned['labels']
        self.bow = np.load(path + '.npz')['bow']

    def train(self, data: Dict):
        '''
        train model from the given dataset. dataset format:
        Dict(
            Label -> Token-Sequence
        )
        '''
        # Collect global vocabulary and setup per-label counters
        vocab, allwc = Counter(), dict()
        for label, sequence in data.items():
            wc = Counter()
            for ngram in nltk.everygrams(sequence, max_len=3):
                vocab.update((ngram,))
                wc.update((ngram,))
            allwc[label] = wc
        self.vocab = list(sorted(vocab.keys()))
        self.labels = list(sorted(data.keys()))
        # finalize bag-of-words vectors
        bow = sparse.dok_matrix((len(data), len(vocab)), dtype=np.int)
        v2j = {v: j for (j,v) in enumerate(self.vocab)}
        for (i, label) in enumerate(self.labels):
            for (k, v) in allwc[label].items():
                bow[i, v2j[k]] = v
        self.bow = bow
        # training statistics
        print('Labels:')
        pprint.pprint(self.labels)
        print('Vocabulary Size:', len(self.vocab), 'Sum:', sum(vocab.values()),
                'Min:', min(vocab.values()), 'Max:', max(vocab.values()))
        x, y = np.histogram(list(vocab.values()), bins=20)
        ncumy = (y/y.sum()).cumsum()
        print('Vocabulary Histogram:')
        for i in range(len(x)):
            print('\t', '%3d'%i, '%6d'%x[i], '%12.3f'%y[i], '%5.3f'%ncumy[i])
        print('BoW Matrix | Shape:', bow.shape, 'Sum:', bow.sum(),
                'Nonzero:', bow.nonzero()[0].size,
                'Density:', bow.nonzero()[0].size / np.prod(bow.shape))

    def readTokens(self, path: str) -> List:
        '''
        read a text file and turn it into a token sequence.
        the tokenization could be improved. NLTK's tokenizer requires
        pre-trained model. blingfire looks like a good candidate but
        that project is still in the early stage.
        '''
        with open(path, 'rt') as f:
            text = re.sub('\W', ' ', f.read()).lower()
        tokens = text.split()
        return tokens

    def train_datadir(self, datadir: str):
        '''
        Train the model from the given directory
        '''
        print(f'Collecting data from {datadir}/* ...')
        files, data = glob.glob(f'{datadir}/*'), dict()
        for f in files:
            tokens = self.readTokens(f)
            data[os.path.basename(f)] = tokens
            print(f'{len(tokens)} ', end='')
        print()
        print('Training model ...')
        self.train(data)

    def predict(self, path: str, topK: int = 1, reject: float = 0.8):
        '''
        compare the given file with the learned data.
        unseen words will simply be discarded.
        '''
        if not self.vocab:
            print("Unknown (100%; Untrained)")
            exit()
        tokens = self.readTokens(path)
        v2j = {v: j for (j,v) in enumerate(self.vocab)}
        wc = Counter()
        for ngram in nltk.everygrams(tokens, max_len=3):
            if ngram in v2j:
                wc.update((ngram,))
        vec = sparse.dok_matrix((len(v2j), 1))
        denom = math.sqrt(sum(x ** 2 for x in wc.values()))
        for (k, v) in wc.items():
            vec[v2j[k], 0] = v / denom
        unitbow = self.bow / np.sqrt((self.bow ** 2).sum(1)).reshape(len(self.labels), 1)
        scores = (unitbow @ vec).ravel()
        sims = (scores * 100).astype(np.int)
        # do we reject?
        if scores.max() < reject:
            print(f'Unknown (Rejected; Max Similarity {sims.max()}%)')
        else:
            # report topK results
            for i in scores.argsort()[::-1][:topK]:
                print(self.labels[i], f'(Similarity: {sims[i]}%)')


if __name__ == '__main__':
    ag = argparse.ArgumentParser()
    ag.add_argument('--train', type=str, default='',
            help='Train the model from the given directory')
    ag.add_argument('--savepath', type=str, default='model',
            help='Where to store the trained model (json,npz)')
    ag.add_argument('--predict', type=str, default='',
            help='Try to classify the given file')
    ag.add_argument('--model', type=str, default='model',
            help='Location of pre-trained model (json,npz)')
    ag.add_argument('--topk', type=int, default=1,
            help='Print top-K similar results')
    ag.add_argument('--reject', type=float, default=0.8,
            help='Threshold under which the model rejects to classify')
    ag.add_argument('--verbose', action='store_true')
    ag = ag.parse_args()

    if ag.train and ag.predict:
        raise Exception('cannot do both at the same time')
    if ag.train:
        model = Model()
        model.train_datadir(ag.train)
        model.save(ag.savepath)
    if ag.predict:
        model = Model()
        if not os.path.exists(ag.model + '.json'):
            ag.model = '/usr/share/doc/duprkit/examples/license-cls-knn'
        model.load(ag.model)
        model.predict(ag.predict, topK=ag.topk, reject=ag.reject)
