#!/usr/bin/python3
# flinkV: duprkit N-gram bag-of-words vector cosine similarity utility
# Copyright (C) 2019 M. Zhou <lumin@debian.org>
# License: MIT/Expat
from typing import *
import argparse, re, os, sys, json, glob, math, random, pprint
from collections import Counter, defaultdict
import numpy as np
'''
Licensecheck-NG
===============

Currently a (toy) license classifier based on k-NN. k-NN is a very simple
machine learning algorithm. In this implementation, we use n-gram bag-of-words
vectors to represent text, and measure text similarity by cosine distance.
The implimentation is pure python and doesn't use anything outside the python
standard library.

Many details could be further improved.
Open an issue on the salsa repo for anything you want to say.

Key words: k-NN, Bag-of-Words, n-Gram, Machine Learning, Computational Linguistics

## Known problems

1. this toy is still not properly assessed. people on IRC suggested me test
it against chromium, fpc, lazarus, boost

2. currently it works not quite well with very short license declarations,
such as the one on the python script itself `:-(`.

3. we need choose a threshold under which the program rejects to classify.

## Dependency

Just `pypy3`. The software is written in pure python and does not use anything
outside the standard library. At this stage we just try to let it work
correctly. Speed optimization? Not today.

## Automatic Training and Assessment on the training set

```
make train
make validate  # testing on training dataset generates a confusion matrix
```

It takes about 1 seconds to train. Very fast.

## Manual training and prediction

Training:

```
./licensecheck-ng.py --train data
```

It will write the trained model to `model.pkl`.

Predict:

```
./licensecheck-ng.py --predict <MY_FILE>
```

It requires `./model.pkl` to be present.

## Comparison to related works

https://wiki.debian.org/CopyrightReviewTools

Well, can I beat all of them? I have no answer currently.

* licensecheck
* scan-copyrights
* cme
* licensecheck2dep5
* license-reconcile
* debmake

Many thanks to Osamu Aoki who provided many training data in debmake's source.

* decopy
* license
* check-all-the-things
* cargo-lichking
* python-debian
* license finder
* licensed
* ninka
* scancode
* dlt
* deb-pkg-tools
* jninka
* apache-rat
* fossology
* OSLCv3
* https://github.com/nexB/scancode-toolkit

The core algorithm is similar to what scancode-toolkit called "match set":
https://github.com/nexB/scancode-toolkit/blob/develop/src/licensedcode/match_set.py

## FAQ

1. it doesn't recognize XXX license.

Training data is simply a bunch of plaintext licenses. Copy the license
content to data/XXX and train the model. Then the model will recognize
the XXX license.

2. it's accuracy is low on some special cases.

Currently the result is produced by the pure algorithm, and there is
no any engineering tweaks. Accuracy in special cases could be further
improved with those tweaks.

3. I want to compare xxx text files (or code) instead of license texts.

Train with your custom text and predict any plain text with the model.

4. why do you try writing such a tool?

Firstly just for fun.
Secondly as a part of the DUPR toolkit (https://github.com/dupr/duprkit)
'''

'''latex
\section{Informal Problem/Solution Formulation}

Given a sequence of tokens $t = [t_1, t_2, \ldots, t_M]$ whose license type
is unknown. We want to identify the license type $c$ from this sequence.
We solve this problem with k-NN ($k=1$) based on $n$-gram bag-of-words text
representations. Specifically, we use unigram, bigram and trigram.

Training: Given a set of $N$ known licenses $D=\{(C_i,T_i)\}_N, 1<=i<=N$
where $C_i$ is the unique name of the license, and $t$ is the token sequence
$ T_i = [t_1, t_2, \ldots, t_M] $. We train a k-NN (k=1) with the 1- and
2-gram bag-of-words representations from dataset $D$.

Predict: Given an unseen token sequence $T_x = [t_1, t_2, \ldots, t_M]$
where unseen tokens were already filtered out, we classify the sequence
$T_x$ by the similarity score $S$. Specifically, we denote the vector
representations of two sequences to be compared with $v_i$ and $v_j$,
then the similarity score $S$ is defined as:
$$ S = 0.5*cos(v_{i,1gram}, v_{j,1gram}) + 0.5*cos(v_{i,2gram}, v_{j,2gram}) $$
The score $S$ will fall in the range $[0,1]$. And the higher the score is,
the more similar the two sequences are.
'''


class Model(object):
    '''
    Model that leverages N-gram Bag-of-Words Representation. Vectors are
    compared with k-NN (k=1) with cosine distance as the metric.
    '''
    def __init__(self):
        # n-gram vocab lists. ordered lists of (strings, or list of strings).
        self.vocab_1gram = list()
        self.vocab_2gram = list()
        self.vocab_3gram = list()
        # bag-of-words vectors for licenses. maps a license-name to a vector.
        self.vectors_1gram = dict()
        self.vectors_2gram = dict()
        self.vectors_3gram = dict()

    def save(self, path: str, verbose: bool = False):
        '''
        save the model to <path>
        '''
        learned = dict()
        if verbose:
            print('Random 1-Gram samples')
            pprint.pprint(random.sample(self.vocab_1gram, 10))
            print('Random 2-Gram samples')
            pprint.pprint(random.sample(self.vocab_2gram, 10))
            print('Random 3-Gram samples')
            pprint.pprint(random.sample(self.vocab_3gram, 10))
        learned['vocab_1gram'] = self.vocab_1gram
        learned['vocab_2gram'] = self.vocab_2gram
        learned['vocab_3gram'] = self.vocab_3gram
        learned['vectors_1gram'] = self.vectors_1gram
        learned['vectors_2gram'] = self.vectors_2gram
        learned['vectors_3gram'] = self.vectors_3gram
        with open(path, 'wt') as f:
            json.dump(learned, f, indent=2)

    def load(self, path: str, verbose: bool = False):
        '''
        load the model from <path>
        '''
        with open(path, 'rt') as f:
            learned = json.load(f)
        self.vocab_1gram = learned['vocab_1gram']
        self.vocab_2gram = list(tuple(x) for x in learned['vocab_2gram'])
        self.vocab_3gram = list(tuple(x) for x in learned['vocab_3gram'])
        self.vectors_1gram = learned['vectors_1gram']
        self.vectors_2gram = learned['vectors_2gram']
        self.vectors_3gram = learned['vectors_3gram']
        if verbose:
            print('Random 1-Gram samples')
            pprint.pprint(random.sample(self.vocab_1gram, 10))
            print('Random 2-Gram samples')
            pprint.pprint(random.sample(self.vocab_2gram, 10))
            print('Random 3-Gram samples')
            pprint.pprint(random.sample(self.vocab_3gram, 10))

    def train(self, data: Dict):
        '''
        train model from the given dataset. dataset format:
        Dict(
            License-Name -> Token-Sequence
        )
        '''
        # Collect vocabulary and Memorize vectors
        dc1g, dc2g, dc3g = dict(), dict(), dict()  # d(ict)c(ounter)
        vo1g, vo2g, vo3g = set(), set(), set()  # vo(cab)
        # go through data
        for k, v in data.items():
            if len(v) < 3:
                raise Exception('Token sequence too short!')
            c1g, c2g, c3g = Counter(), Counter(), Counter()
            for i in range(2, len(v)):
                unigram, bigram = [v[i]], [(v[i-1], v[i])]
                trigram = [(v[i-2], v[i-1], v[i])]
                c1g.update(unigram)
                c2g.update(bigram)
                c3g.update(trigram)
                vo1g.update(unigram)
                vo2g.update(bigram)
                vo3g.update(trigram)
            dc1g[k], dc2g[k], dc3g[k] = c1g, c2g, c3g
        # finalize vocabulary list
        self.vocab_1gram = list(sorted(vo1g))
        self.vocab_2gram = list(sorted(vo2g))
        self.vocab_3gram = list(sorted(vo3g))
        # finalize bag-of-words vectors
        dv1g, dv2g, dv3g = dict(), dict(), dict()  # d(ict)v(ector)
        for k in data.keys():
            v1g, v2g, v3g = [0]*len(vo1g), [0]*len(vo2g), [0]*len(vo3g)
            for (i, word) in enumerate(self.vocab_1gram):
                v1g[i] = dc1g[k].get(word, 0)
            for (i, word) in enumerate(self.vocab_2gram):
                v2g[i] = dc2g[k].get(word, 0)
            for (i, word) in enumerate(self.vocab_3gram):
                v3g[i] = dc3g[k].get(word, 0)
            dv1g[k], dv2g[k], dv3g[k] = v1g, v2g, v3g
        self.vectors_1gram = dv1g
        self.vectors_2gram = dv2g
        self.vectors_3gram = dv3g
        # statistics
        print('Vocabulary Size:',
                '1-Gram[', len(self.vocab_1gram), ']',
                '2-Gram[', len(self.vocab_2gram), ']',
                '3-Gram[', len(self.vocab_3gram), ']')
        for k in data.keys():
            print(k)
            print('\t1-Gram |',
                'Sum[', np.sum(self.vectors_1gram[k]), ']',
                'Max[', np.max(self.vectors_1gram[k]), ']',
                'Mean[', np.mean(self.vectors_1gram[k]), ']')
            print('\t2-Gram |',
                'Sum[', np.sum(self.vectors_2gram[k]), ']',
                'Max[', np.max(self.vectors_2gram[k]), ']',
                'Mean[', np.mean(self.vectors_2gram[k]), ']')
            print('\t3-Gram |',
                'Sum[', np.sum(self.vectors_3gram[k]), ']',
                'Max[', np.max(self.vectors_3gram[k]), ']',
                'Mean[', np.mean(self.vectors_3gram[k]), ']')

    def readTokens(self, path: str) -> List:
        '''
        read a text file and turn it into a token sequence.
        the tokenization could be improved.
        '''
        with open(path, 'rt') as f:
            text = re.sub('\W', ' ', f.read()).lower()
        tokens = text.split()
        return tokens

    def train_datadir(self, datadir: str):
        '''
        Train the model from the given directory
        '''
        print(f'Collecting data from {datadir}/* ...')
        files, data = glob.glob(f'{datadir}/*'), dict()
        for f in files:
            tokens = self.readTokens(f)
            data[os.path.basename(f)] = tokens
            print(f'{len(tokens)} ', end='')
        print()
        print('Training model ...')
        self.train(data)

    def XcosSim(self, vecA: Counter, vecB: Counter) -> float:
        '''
        cosine similarity
        '''
        keys = set(vecA.keys()).union(set(vecB.keys()))
        denomA = math.sqrt(sum(x**2 for x in vecA.values()))
        denomB = math.sqrt(sum(x**2 for x in vecB.values()))
        vA, vB = [], []
        for k in keys:
            vA.append(vecA.get(k, 0) / float(denomA))
            vB.append(vecB.get(k, 0) / float(denomB))
        dot = [vA[i]*vB[i] for i in range(len(vA))]
        return sum(dot)

    def predict(self, path: str, topK: int = 1, reject: float = 0.8):
        '''
        compare the given file with the learned data.
        unseen words will simply be discarded.
        '''
        if not self.vocab_1gram:
            raise Exception("An un-trained model cannot predict.")
        tokens = self.readTokens(path)
        if len(tokens) < 3:
            raise Exception("Token sequence too short!")
        v1g = np.zeros(len(self.vocab_1gram))
        v2g = np.zeros(len(self.vocab_2gram))
        v3g = np.zeros(len(self.vocab_3gram))
        for i in range(2, len(tokens)):
            unigram, bigram = tokens[i], (tokens[i-1], tokens[i])
            trigram = (tokens[i-2], tokens[i-1], tokens[i])
            if unigram in self.vocab_1gram:
                v1g[self.vocab_1gram.index(unigram)] += 1
            if bigram in self.vocab_2gram:
                v2g[self.vocab_2gram.index(bigram)] += 1
            if trigram in self.vocab_3gram:
                v3g[self.vocab_3gram.index(trigram)] += 1
        # put the vector onto the unit hypersphere
        v1g = v1g / np.sqrt(np.power(v1g, 2).sum())
        v2g = v2g / np.sqrt(np.power(v2g, 2).sum())
        v3g = v3g / np.sqrt(np.power(v3g, 2).sum())
        # get the training set matrices and put them onto the hypersphere
        licenses = list(sorted(self.vectors_1gram.keys()))
        t1g = np.vstack([self.vectors_1gram[k] for k in licenses])
        t1g = t1g / np.sqrt(np.power(t1g, 2).sum(1).reshape(t1g.shape[0], 1))
        t2g = np.vstack([self.vectors_2gram[k] for k in licenses])
        t2g = t2g / np.sqrt(np.power(t2g, 2).sum(1).reshape(t2g.shape[0], 1))
        t3g = np.vstack([self.vectors_3gram[k] for k in licenses])
        t3g = t3g / np.sqrt(np.power(t3g, 2).sum(1).reshape(t3g.shape[0], 1))
        # let's aggregate and compare
        scores1g = t1g @ v1g
        scores2g = t2g @ v2g
        scores3g = t3g @ v3g
        scores = (scores1g + scores2g + scores3g)/3.0
        sims = (scores * 100).astype(np.int)
        # do we reject?
        if scores.max() < reject:
            print(f'Unknown (Rejected; Max Similarity {sims.max()}%)')
        # report topK results
        for i in scores.argsort()[::-1][:topK]:
            print(licenses[i], f'(Similarity: {sims[i]}%)')


if __name__ == '__main__':
    ag = argparse.ArgumentParser()
    ag.add_argument('--train', type=str, default='',
            help='Train the model from the given directory')
    ag.add_argument('--savepath', type=str, default='model.json',
            help='Where to store the trained model (json)')
    ag.add_argument('--predict', type=str, default='',
            help='Try to classify the given file')
    ag.add_argument('--model', type=str, default='model.json',
            help='Location of pre-trained model (json)')
    ag.add_argument('--topk', type=int, default=1,
            help='Print top-K similar results')
    ag.add_argument('--reject', type=float, default=0.8,
            help='Threshold under which the model rejects to classify')
    ag.add_argument('--verbose', action='store_true')
    ag = ag.parse_args()

    if ag.train and ag.predict:
        raise Exception('cannot do both at the same time')
    if ag.train:
        model = Model()
        model.train_datadir(ag.train)
        model.save(ag.savepath, verbose=ag.verbose)
    if ag.predict:
        model = Model()
        if not os.path.exists(ag.model):
            ag.model = '/usr/share/doc/duprkit/examples/license-cls-knn.json'
        model.load(ag.model, verbose=ag.verbose)
        model.predict(ag.predict, topK=ag.topk, reject=ag.reject)
