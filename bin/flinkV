#!/usr/bin/python3
# flinkV: N-gram bag-of-words vector cosine similarity utility
# Copyright (C) 2019 M. Zhou <lumin@debian.org>
# License: MIT/Expat
from typing import *
import argparse, re, os, sys, json, glob, math, random, pprint
from collections import Counter, defaultdict
import numpy as np
from scipy import sparse
import nltk
'''
flinkV implements a (toy) text classifier with the k-NN model (k=1), using
N-gram bag-of-words representation vectors as text representation, and cosine
similarity as the metric.

Keywords: k-NN (machine learning), Bag-of-Words (computational linguistics),
          N-gram (computational linguistics), vector space (linear algebra)

#### Problem Formulation

```latex
Given a sequence of tokens $t = [t_1, t_2, \ldots, t_M]$, we want to classify
the sequence and produce a corresponding output $c$.
```

## Known problems

1. this toy is still not properly assessed. people on IRC suggested me test
it against chromium, fpc, lazarus, boost

2. currently it works not quite well with very short license declarations,
such as the one on the python script itself `:-(`.

3. we need choose a threshold under which the program rejects to classify.

## Manual training and prediction

./licensecheck-ng.py --train datadir/
./licensecheck-ng.py --predict my_file

## Comparison to related works

https://wiki.debian.org/CopyrightReviewTools

* licensecheck
* scan-copyrights
* cme
* licensecheck2dep5
* license-reconcile
* debmake
* decopy
* license
* check-all-the-things
* cargo-lichking
* python-debian
* license finder
* licensed
* ninka
* scancode
* dlt
* deb-pkg-tools
* jninka
* apache-rat
* fossology
* OSLCv3
* https://github.com/nexB/scancode-toolkit
  The core algorithm is similar to what scancode-toolkit called "match set":
  https://github.com/nexB/scancode-toolkit/blob/develop/src/licensedcode/match_set.py
'''

class Model(object):
    '''
    Model that leverages N-gram Bag-of-Words Representation. Vectors are
    compared with k-NN (k=1) with cosine distance as the metric.
    '''
    def __init__(self):
        # Ordered vocab list of every-grams.
        # exampe: [('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
        self.vocab = list()
        # labels, len(labels)==C . Each label corresponds to a row of bow matrix.
        self.labels = list()
        # matrix formed by every-grams bag-of-words vectors. matrix.shape[0]==C
        self.bow = sparse.dok_matrix((1,1), dtype=np.int)

    def save(self, name: str):
        '''
        save the model to <name>.json and <name>.npz
        '''
        with open(name + '.json', 'wt') as f:
            json.dump({'vocab': self.vocab, 'labels': self.labels}, f, indent=2)
        sparse.save_npz(name + '.npz', self.bow)

    def load(self, path: str):
        '''
        load the model from <path>.json and <path>.npz
        '''
        with open(path + '.json', 'rt') as f:
            learned = json.load(f)
        self.vocab = list(tuple(x) for x in learned['vocab'])
        self.labels = learned['labels']

    def train(self, data: Dict):
        '''
        train model from the given dataset. dataset format:
        Dict(
            License-Name -> Token-Sequence
        )
        '''
        # Collect vocabulary and Memorize vectors
        dc1g, dc2g, dc3g = dict(), dict(), dict()  # d(ict)c(ounter)
        vo1g, vo2g, vo3g = set(), set(), set()  # vo(cab)
        # go through data
        for k, v in data.items():
            if len(v) < 3:
                raise Exception('Token sequence too short!')
            c1g, c2g, c3g = Counter(), Counter(), Counter()
            for i in range(2, len(v)):
                unigram, bigram = [v[i]], [(v[i-1], v[i])]
                trigram = [(v[i-2], v[i-1], v[i])]
                c1g.update(unigram)
                c2g.update(bigram)
                c3g.update(trigram)
                vo1g.update(unigram)
                vo2g.update(bigram)
                vo3g.update(trigram)
            dc1g[k], dc2g[k], dc3g[k] = c1g, c2g, c3g
        # finalize vocabulary list
        self.vocab_1gram = list(sorted(vo1g))
        self.vocab_2gram = list(sorted(vo2g))
        self.vocab_3gram = list(sorted(vo3g))
        # finalize bag-of-words vectors
        dv1g, dv2g, dv3g = dict(), dict(), dict()  # d(ict)v(ector)
        for k in data.keys():
            v1g, v2g, v3g = [0]*len(vo1g), [0]*len(vo2g), [0]*len(vo3g)
            for (i, word) in enumerate(self.vocab_1gram):
                v1g[i] = dc1g[k].get(word, 0)
            for (i, word) in enumerate(self.vocab_2gram):
                v2g[i] = dc2g[k].get(word, 0)
            for (i, word) in enumerate(self.vocab_3gram):
                v3g[i] = dc3g[k].get(word, 0)
            dv1g[k], dv2g[k], dv3g[k] = v1g, v2g, v3g
        self.vectors_1gram = dv1g
        self.vectors_2gram = dv2g
        self.vectors_3gram = dv3g
        # statistics
        print('Vocabulary Size:',
                '1-Gram[', len(self.vocab_1gram), ']',
                '2-Gram[', len(self.vocab_2gram), ']',
                '3-Gram[', len(self.vocab_3gram), ']')
        for k in data.keys():
            print(k)
            print('\t1-Gram |',
                'Sum[', np.sum(self.vectors_1gram[k]), ']',
                'Max[', np.max(self.vectors_1gram[k]), ']',
                'Mean[', np.mean(self.vectors_1gram[k]), ']')
            print('\t2-Gram |',
                'Sum[', np.sum(self.vectors_2gram[k]), ']',
                'Max[', np.max(self.vectors_2gram[k]), ']',
                'Mean[', np.mean(self.vectors_2gram[k]), ']')
            print('\t3-Gram |',
                'Sum[', np.sum(self.vectors_3gram[k]), ']',
                'Max[', np.max(self.vectors_3gram[k]), ']',
                'Mean[', np.mean(self.vectors_3gram[k]), ']')

    def readTokens(self, path: str) -> List:
        '''
        read a text file and turn it into a token sequence.
        the tokenization could be improved. NLTK's tokenizer requires
        pre-trained model. blingfire looks like a good candidate but
        that project is still in the early stage.
        '''
        with open(path, 'rt') as f:
            text = re.sub('\W', ' ', f.read()).lower()
        tokens = text.split()
        return tokens

    def train_datadir(self, datadir: str):
        '''
        Train the model from the given directory
        '''
        print(f'Collecting data from {datadir}/* ...')
        files, data = glob.glob(f'{datadir}/*'), dict()
        for f in files:
            tokens = self.readTokens(f)
            data[os.path.basename(f)] = tokens
            print(f'{len(tokens)} ', end='')
        print()
        print('Training model ...')
        self.train(data)

    def predict(self, path: str, topK: int = 1, reject: float = 0.8):
        '''
        compare the given file with the learned data.
        unseen words will simply be discarded.
        '''
        if not self.vocab_1gram:
            raise Exception("An un-trained model cannot predict.")
        tokens = self.readTokens(path)
        if len(tokens) < 3:
            raise Exception("Token sequence too short!")
        v1g = np.zeros(len(self.vocab_1gram))
        v2g = np.zeros(len(self.vocab_2gram))
        v3g = np.zeros(len(self.vocab_3gram))
        for i in range(2, len(tokens)):
            unigram, bigram = tokens[i], (tokens[i-1], tokens[i])
            trigram = (tokens[i-2], tokens[i-1], tokens[i])
            if unigram in self.vocab_1gram:
                v1g[self.vocab_1gram.index(unigram)] += 1
            if bigram in self.vocab_2gram:
                v2g[self.vocab_2gram.index(bigram)] += 1
            if trigram in self.vocab_3gram:
                v3g[self.vocab_3gram.index(trigram)] += 1
        # put the vector onto the unit hypersphere
        v1g = v1g / np.sqrt(np.power(v1g, 2).sum())
        v2g = v2g / np.sqrt(np.power(v2g, 2).sum())
        v3g = v3g / np.sqrt(np.power(v3g, 2).sum())
        # get the training set matrices and put them onto the hypersphere
        licenses = list(sorted(self.vectors_1gram.keys()))
        t1g = np.vstack([self.vectors_1gram[k] for k in licenses])
        t1g = t1g / np.sqrt(np.power(t1g, 2).sum(1).reshape(t1g.shape[0], 1))
        t2g = np.vstack([self.vectors_2gram[k] for k in licenses])
        t2g = t2g / np.sqrt(np.power(t2g, 2).sum(1).reshape(t2g.shape[0], 1))
        t3g = np.vstack([self.vectors_3gram[k] for k in licenses])
        t3g = t3g / np.sqrt(np.power(t3g, 2).sum(1).reshape(t3g.shape[0], 1))
        # they are actually sparse matrices
        #print(t1g.nonzero()[0].size / t1g.size)
        #print(t2g.nonzero()[0].size / t2g.size)
        #print(t3g.nonzero()[0].size / t3g.size)
        # let's aggregate and compare
        t1g = sparse.dok_matrix(t1g)
        t2g = sparse.dok_matrix(t2g)
        t3g = sparse.dok_matrix(t3g)
        scores1g = t1g @ v1g
        scores2g = t2g @ v2g
        scores3g = t3g @ v3g
        scores = (scores1g + scores2g + scores3g)/3.0
        sims = (scores * 100).astype(np.int)
        # do we reject?
        if scores.max() < reject:
            print(f'Unknown (Rejected; Max Similarity {sims.max()}%)')
        else:
            # report topK results
            for i in scores.argsort()[::-1][:topK]:
                print(licenses[i], f'(Similarity: {sims[i]}%)')


if __name__ == '__main__':
    ag = argparse.ArgumentParser()
    ag.add_argument('--train', type=str, default='',
            help='Train the model from the given directory')
    ag.add_argument('--savepath', type=str, default='model.json',
            help='Where to store the trained model (json)')
    ag.add_argument('--predict', type=str, default='',
            help='Try to classify the given file')
    ag.add_argument('--model', type=str, default='model.json',
            help='Location of pre-trained model (json)')
    ag.add_argument('--topk', type=int, default=1,
            help='Print top-K similar results')
    ag.add_argument('--reject', type=float, default=0.8,
            help='Threshold under which the model rejects to classify')
    ag.add_argument('--verbose', action='store_true')
    ag = ag.parse_args()

    if ag.train and ag.predict:
        raise Exception('cannot do both at the same time')
    if ag.train:
        model = Model()
        model.train_datadir(ag.train)
        model.save(ag.savepath, verbose=ag.verbose)
    if ag.predict:
        model = Model()
        if not os.path.exists(ag.model):
            ag.model = '/usr/share/doc/duprkit/examples/license-cls-knn.json'
        model.load(ag.model, verbose=ag.verbose)
        model.predict(ag.predict, topK=ag.topk, reject=ag.reject)
